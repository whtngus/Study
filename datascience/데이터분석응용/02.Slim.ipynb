{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slim\n",
    "\n",
    "- Keras 와 비슷한 점을 지향\n",
    "- network 구조를 효율적으로 사용하기 위해 만들어짐\n",
    "\n",
    "### 그럼 왜 keras를 안쓸고 slim을 사용할까?\n",
    "\n",
    "- slim의 독특한 특징\n",
    "- slim base의 pre-trained model 이 많음\n",
    "\n",
    "### 실습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# contrib는  Tensorflow 1v 베타  2v부터는 사라짐\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- device 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.contrib.framework.python.ops.variables.variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, device=None, partitioner=None, custom_getter=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0f0d318f9f8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# slim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\contrib\\framework\\python\\ops\\variables.py\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1243\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32md:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32md:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[1;32md:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"tensorflow/python\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[1;32m--> 868\u001b[1;33m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"d:\\tool\\conda\\envs\\skku\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "# tf\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[10,10,3,3]),name=\"weights\")\n",
    "\n",
    "# slim\n",
    "weights = slim.variable(\"weights\", shape=[10,10,3,3], initializer=tf.truncated_normal_initializer,device=\"/cpu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable 의 종류\n",
    "1. variable\n",
    "모델을 저장하고 싶을때, save를 통해서 저장이 되는 변수\n",
    "2. local variable\n",
    "session이 살아있는 동안만 존재하는 변수\n",
    "\n",
    "3. slim 은 model variable 을 하나더 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'weights:0' shape=(10, 10, 3, 3) dtype=float32_ref>]\n",
      "[<tf.Variable 'weights:0' shape=(10, 10, 3, 3) dtype=float32_ref>, <tf.Variable 'my_var:0' shape=(10, 10, 3, 3) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "weights = slim.model_variable(\"weights\", shape=[10,10,3,3], initializer=tf.truncated_normal_initializer,device=\"/cpu:0\")\n",
    "weights = slim.variable(\"my_var\", shape=[10,10,3,3], initializer=tf.truncated_normal_initializer,device=\"/cpu:0\")\n",
    "print(slim.get_model_variables()) # 모델 변수 호출\n",
    "print(slim.get_variables()) #모든 변수 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slim 의 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'repeat_1/conv3_1/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3_1/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3_2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3_2/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3_3/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3_3/biases:0' shape=(256,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에러가 나는 경우 pip install gast==0.2.2\n",
    "tf.reset_default_graph()\n",
    "net = tf.placeholder(tf.float32,[16,32,32,256])\n",
    "with tf.variable_scope(\"repeat_1\"):\n",
    "    net = slim.conv2d(net,256,[3,3],scope=\"conv3_1\")\n",
    "    net = slim.conv2d(net,256,[3,3],scope=\"conv3_2\")\n",
    "    net = slim.conv2d(net,256,[3,3],scope=\"conv3_3\")\n",
    "    net = slim.max_pool2d(net,[2,2],scope=\"pool2\")\n",
    "# 겹치는 모델 설정을 scope 범위를 사용하여 지정할 수 있음\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"repeat_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'repeat_1/conv3/conv3_1/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_1/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_2/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_3/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_3/biases:0' shape=(256,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "net = tf.placeholder(tf.float32,[16,32,32,256])\n",
    "with tf.variable_scope(\"repeat_1\"):\n",
    "    # + 반복 모델을 for문대신 사용하여 편하게 사용 가능\n",
    "    net = slim.repeat(net,3,slim.conv2d,256,[3,3],scope=\"conv3\")\n",
    "    net = slim.max_pool2d(net,[2,2],scope=\"pool2\")\n",
    "# 겹치는 모델 설정을 scope 범위를 사용하여 지정할 수 있음\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"repeat_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파라미터가 다른경우의 repet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'repeat_1/conv3/conv3_1/weights:0' shape=(3, 3, 256, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_1/biases:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_2/weights:0' shape=(2, 2, 32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_2/biases:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_3/weights:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_3/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_4/weights:0' shape=(2, 2, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'repeat_1/conv3/conv3_4/biases:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "net = tf.placeholder(tf.float32,[16,32,32,256])\n",
    "with tf.variable_scope(\"repeat_1\"):\n",
    "    # + 반복 모델을 for문대신 사용하여 편하게 사용 가능\n",
    "    net = slim.stack(net,slim.conv2d,[(32,[3,3]),(32,[2,2]),(64,[3,3]),(64,[2,2])],scope=\"conv3\")\n",
    "    net = slim.max_pool2d(net,[2,2],scope=\"pool2\")\n",
    "# 겹치는 모델 설정을 scope 범위를 사용하여 지정할 수 있음\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"repeat_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scope\n",
    "layer를 쌓으면서 겹치는 argument가 많은경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "padding = \"SAME\"\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "regularizer = slim.l2_regularizer(0.0005)\n",
    "\n",
    "inputs = tf.placeholder(tf.float32,[16,224,224,3])\n",
    "ner = slim.conv2d(inputs,64,[11,11],4,padding=padding, weights_regularizer=regularizer, weights_initializer=initializer,scope=\"conv1\")\n",
    "ner = slim.conv2d(ner,64,[11,11],128,padding=padding, weights_regularizer=regularizer, weights_initializer=initializer,scope=\"conv2\")\n",
    "ner = slim.conv2d(ner,64,[11,11],256,padding=padding, weights_regularizer=regularizer, weights_initializer=initializer,scope=\"conv3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slim으로 교체\n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32,[16,224,224,3])\n",
    "with slim.arg_scope([slim.conv2d],padding=\"SAME\",\n",
    "                  weights_regularizer=slim.l2_regularizer(0.0005),\n",
    "                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01)):\n",
    "    ner = slim.conv2d(inputs,64,[11,11],4,scope=\"conv1\")\n",
    "    ner = slim.conv2d(ner,64,[11,11],128,scope=\"conv2\")\n",
    "    ner = slim.conv2d(ner,64,[11,11],256,scope=\"conv3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scope 는 2중 3중으로도 가능하다 <br>\n",
    "코드 설명 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slim의 training\n",
    "\n",
    "- loss의 더하기  전부 가져오기 가능\n",
    "ex) <br>\n",
    "slim.losses.get_total_loss(add_regularization_losses=False) <br>\n",
    "\n",
    "- train op 과 update op\n",
    "트레이닝(백프로파게이션)으로 업데이트 하지 않는 파라미터의 경우, tf에서는 control flow를 이용하여 control dependency를 통하여 업데이트를 할 수 있다. <br>\n",
    "-> 난이도가 매우 어려움  <br>\n",
    "slim.learning.create_train_op(total_loss, optimizer) -> 두가지를 동시에 사용 <br>\n",
    "\n",
    "=> Tensorflow v2 에서는 해당 기능이 필요하지 않음 <br>\n",
    "\n",
    "- Fine-Tuning <br>\n",
    "내가 트레이닝한 그래프와 다른사람이 트레이닝한 그래프가 일치하지 않는경우 ... <br>\n",
    "variable restore할지를 결정해야 하기 때문에 variable name을 하나씩 가져와야함  <br>\n",
    "slim 은 여러가지 가져올 수 있는 방법을 제공 <br>\n",
    "slim.get_variables_by_name <br> \n",
    "slim.get_variables_by_suffix <br>\n",
    "get_variables <br>\n",
    "get_variables_to_restore(include=[\" 값 \"])  <br>\n",
    "get_variables_to_restore(exclude=\" 값 \"]) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
